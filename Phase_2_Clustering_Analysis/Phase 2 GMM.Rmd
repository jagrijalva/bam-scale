---
title: "Phase 2 GMM (new)"
author: "Jessala Grijalva"
date: "`r Sys.Date()`"
output: html_document
---

# Introduction

## Setup and Preliminaries

Establishes the environment, loads necessary libraries, and sets the seed for reproducibility. Loads and prepares data for analysis, which is a crucial first step in any data analysis process.

### Libraries and Seed

```{r}
# Load necessary libraries
library(pacman)
p_load(mclust, cluster, dplyr, tidyverse, factoextra, flexclust, caret, 
       dunn.test, MASS, Rtsne, NbClust, boot, fpc, ggplot2, gridExtra, grid,
       kableExtra, rstatix, PMCMRplus, FSA, DescTools, mclust, stats, purrr, 
       fmsb)

# Set seed for reproducibility
set.seed(2500)
```

### Data Loading and Preparation

```{r}
# Set the working directory to the folder where the RDA file is located
load("~/Desktop/Docs/Research/LNS Project/New/lns_clean.rda")
```

## Clustering Preparation

 Defines variables for clustering and standardizes them, which is necessary to ensure that all variables contribute equally to the analysis.

### Define Clustering Variables

```{r}
# Select variables for K Means clustering
selected_vars <- c("AMERICAN", "CULTURAL_IDENTITY", "KEEPSPAN", "DISTINCT", 
                   "LEARNENG")

# Subset the data for clustering
clustering_data <- lns_subset[, selected_vars]
```

### Standardization

```{r}
# Standardize the selected variables (mean = 0, sd = 1)
clustering_data_standardized <- scale(clustering_data, center = TRUE, scale = TRUE)

# Remove rows with any missing values after standardization
clustering_data_clean <- na.omit(clustering_data_standardized)
```

## Clustering Evaluation Functions

Provides functions to calculate key clustering metrics. This is important to assess the quality of the clustering solution.

```{r}
# Silhouette, Dunn, and Calinski-Harabasz Indices Calculation
# Function to Evaluate Clusters
evaluate_clusters <- function(gmm_obj, data) {
  tryCatch({
    # Calculate Silhouette Score
    silhouette_avg <- mean(silhouette(as.integer(gmm_obj$classification), dist(data))[, 'sil_width'])
    
    # Calculate Dunn Index
    dunn_result <- cluster.stats(dist(data), as.integer(gmm_obj$classification))$dunn
    
    # Calculate Calinski-Harabasz Index
    calinski_harabasz <- cluster.stats(dist(data), as.integer(gmm_obj$classification))$ch
    
    return(list(silhouette_avg = silhouette_avg, 
                dunn_index = dunn_result, 
                calinski_harabasz = calinski_harabasz))
  }, error = function(e) {
    message("An error occurred: ", e)
  })
}
```

## Gaussian Mixture Model (GMM) Analysis

Fits the GMM for different k values and evaluates each model, which is the core part of the clustering analysis.

- Fitting GMM Across Different 'k' Values
- Loop through a set range of `k` values to fit GMM.
- Evaluate each model using the previously defined function.

```{r}
# Set cluster solutions to try
cluster_solutions <- c(3, 4, 5)

# Initialize an empty list to store GMM results
gmm_results <- list()

# Initialize an empty data frame to store summary metrics for quick insights
gmm_summary <- data.frame()

# Loop through each cluster solution
for (k in cluster_solutions) {
  tryCatch({
    # Fit GMM
    gmm_fit <- Mclust(data = clustering_data_clean, G = k)
    
    # Evaluate clusters
    eval_metrics <- evaluate_clusters(gmm_fit, clustering_data_clean)
    
    # Calculate cluster sizes
    cluster_size <- table(gmm_fit$classification)
    
    # Store results in list
    gmm_results[[paste("GMM_k", k, sep = "_")]] <- list(model = gmm_fit, metrics = eval_metrics, sizes = cluster_size)
    
    # Store the metrics and sizes in a data frame for easier comparison
    run_summary <- data.frame(
      Model = paste("GMM_k", k, sep = "_"),
      SilhouetteAvg = eval_metrics$silhouette_avg,
      DunnIndex = eval_metrics$dunn_index,
      CalinskiHarabasz = eval_metrics$calinski_harabasz,
      ClusterSizes = paste(cluster_size, collapse = ", ")
    )
    gmm_summary <- rbind(gmm_summary, run_summary)
    
    cat("\n--- Metrics for GMM with k =", k, "---\n")
    print(eval_metrics)
    cat("Cluster Sizes:\n")
    print(cluster_size)
  }, error = function(e) {
    message("An error occurred for k = ", k, ": ", e)
  })
}

# Print the summary data frame
cat("\n--- Summary of Metrics and Sizes for All GMM Models ---\n")
print(gmm_summary)
```

## GMM Means Summary

Calculates and prints the mean values for the variables in each cluster, which helps in understanding the characteristics of each cluster.

```{r}
# Range of k values to evaluate
k_values <- c(3, 4, 5)

# Loop through each k value
for (k in k_values) {
  # Fit GMM for the current k
  gmm_fit <- Mclust(data = clustering_data_clean, G = k)
  
  # Assign clusters to the original data
  original_data_with_clusters <- data.frame(lns_subset[, selected_vars], cluster = gmm_fit$classification)
  
  # Calculate mean of original variables for each cluster
  cluster_means <- original_data_with_clusters %>%
    group_by(cluster) %>%
    summarise(across(everything(), ~mean(.x, na.rm = TRUE)), .groups = 'drop')
  
  # Print the cluster means
  cat("\nCluster Means for k =", k, ":\n")
  print(cluster_means)
}
```

## Optimizing GMM

This part optimizes the clustering by incorporating results from a preliminary K-means clustering, which can improve the starting conditions for GMM and potentially lead to a more accurate clustering solution.

```{r}
# Perform K-means clustering for a range of cluster numbers and fit GMM
k_values <- 3:5
gmm_fits <- list()
bic_values <- numeric(length(k_values))
silhouette_scores <- numeric(length(k_values))

for (k in k_values) {
  # K-means pre-clustering
  kmeans_result <- kmeans(clustering_data_clean, centers = k, nstart = 25)
  
  # Fit GMM using the initial means from K-means
  gmm_fit <- Mclust(data = clustering_data_clean, G = k, 
                    initializationValue = kmeans_result$centers)
  gmm_fits[[paste("GMM_k", k, sep = "_")]] <- gmm_fit
  
  # Store BIC values
  bic_values[k - min(k_values) + 1] <- gmm_fit$bic
  
  # Calculate silhouette scores
  classification <- gmm_fit$classification
  data_dist <- dist(clustering_data_clean)
  silhouette_info <- silhouette(classification, data_dist)
  silhouette_scores[k - min(k_values) + 1] <- mean(silhouette_info[, "sil_width"])
}

# Report detailed results for each k
cat("Detailed Results for Each k:\n")
for (k in k_values) {
  cat("For k =", k, "\n")
  cat("BIC:", bic_values[k - min(k_values) + 1], "\n")
  cat("Silhouette Score:", silhouette_scores[k - min(k_values) + 1], "\n\n")
}

# Identify and report the best models based on BIC and Silhouette Score
best_bic_index <- which.max(bic_values)
best_silhouette_index <- which.max(silhouette_scores)

cat("Summary of Best Models:\n")
cat("Best model based on BIC has k =", k_values[best_bic_index], "with BIC:", bic_values[best_bic_index], "\n")
cat("Best model based on Silhouette Score has k =", k_values[best_silhouette_index], "with score:", silhouette_scores[best_silhouette_index], "\n")
```

## Robustness Checks

Uses bootstrapping to assess the stability and reliability of the cluster assignments. This is a crucial step in ensuring the clusters are not artifacts of the sample.

```{r}
# Original GMM clustering to get baseline cluster centers for comparison
original_gmm <- Mclust(clustering_data_clean, G = 4)

# Function to align cluster labels based on minimal distance to original centers
align_clusters <- function(boot_centers, original_centers) {
  # Compute all pairwise distances between centers
  dist_matrix <- as.matrix(dist(rbind(boot_centers, original_centers)))
  # Subset to distances between boot and original centers only
  dist_matrix <- dist_matrix[1:nrow(boot_centers), (nrow(boot_centers)+1):nrow(dist_matrix)]
  
  alignment <- apply(dist_matrix, 1, which.min)
  return(alignment)
}

# Function to perform bootstrapping of GMM clustering with alignment
cluster_gmm <- function(data, indices, original_centers) {
  boot_data <- data[indices, ]  # Resampling with replacement
  gmm_result <- Mclust(boot_data, G = 4)  # GMM clustering
  
  # Align the clusters
  alignment <- align_clusters(gmm_result$parameters$mean, original_centers)
  
  # Return aligned cluster assignments
  return(factor(alignment[gmm_result$classification]))
}

# Perform the bootstrap analysis
boot_results <- boot(
  data = clustering_data_clean, 
  statistic = function(data, indices) cluster_gmm(data, indices, original_gmm$parameters$mean), 
  R = 250
)

# Generate summary statistics of bootstrap results
summary_stats <- boot_results$t %>%
  as.data.frame() %>%
  summarise_all(list(mean = mean, sd = sd, median = median))

# Convert the bootstrap results to a data frame
bootstrap_df <- as.data.frame(boot_results$t)

# Calculate summary statistics for each cluster
summary_stats <- sapply(bootstrap_df, function(x) {
  c(mean = mean(x), sd = sd(x), median = median(x))
})

# Convert summary statistics to a data frame for printing
summary_stats_df <- as.data.frame(t(summary_stats))
colnames(summary_stats_df) <- c("Mean", "SD", "Median")
summary_stats_df$Cluster <- 1:ncol(bootstrap_df)

# Printing summary stats
print(summary_stats_df)

# Convert the bootstrap results to a data frame
bootstrap_df <- as.data.frame(boot_results$t)

# Generate summary statistics for each cluster
summary_stats <- lapply(1:4, function(clust) {
  clust_samples <- bootstrap_df[, clust]
  return(c(
    BootBias = mean(clust_samples - clust), 
    BootSE = sd(clust_samples), 
    BootMed = median(clust_samples)
  ))
})

# Convert the list of summaries into a data frame
summary_stats_df <- do.call(rbind, summary_stats)
rownames(summary_stats_df) <- NULL
summary_stats_df <- as.data.frame(summary_stats_df)
summary_stats_df$Cluster <- 1:4

# Printing summary stats
print(summary_stats_df)
```

## Cross-Validation

```{r}
# Generate folds ensuring indices are appropriate for the dataset
folds <- createFolds(1:nrow(clustering_data_clean), k = 5, list = TRUE)

gmm_cluster_and_summarize <- function(train_indices, test_indices, clustering_data, original_data, selected_vars) {
    # Ensure indices are treated as numeric for subsetting
    train_indices <- as.numeric(train_indices)
    test_indices <- as.numeric(test_indices)
    
    # Subset data based on indices
    train_data <- clustering_data[train_indices, ]
    test_data <- clustering_data[test_indices, ]
    
    # Fit GMM on the training data
    gmm_fit <- Mclust(train_data, G = 4)
    
    # Predict clusters for the test data
    test_clusters <- predict(gmm_fit, newdata = test_data)$classification
    
    # Bind the predicted clusters with the original variables from test data
    original_test_data <- original_data[test_indices, selected_vars]
    test_data_clustered <- cbind(original_test_data, cluster = test_clusters)
    
    # Calculate means for each variable within each cluster
    cluster_means <- test_data_clustered %>%
        group_by(cluster) %>%
        summarise(across(everything(), mean, na.rm = TRUE), .groups = 'drop')
    
    return(cluster_means)
}

cv_results <- lapply(folds, function(indices) {
    gmm_cluster_and_summarize(-indices, indices, clustering_data_clean, lns_subset, selected_vars)
})

# Combine all folds into one data frame
combined_cv_results <- bind_rows(cv_results, .id = "Fold")

# Calculate overall mean for each variable in each cluster across all folds
overall_means <- combined_cv_results %>%
  group_by(cluster) %>%
  summarise(across(-Fold, mean, na.rm = TRUE), .groups = 'drop')

# Calculate the standard deviation for each variable in each cluster across all folds, to assess variability
overall_sd <- combined_cv_results %>%
  group_by(cluster) %>%
  summarise(across(-Fold, sd, na.rm = TRUE), .groups = 'drop')

# Printing the overall means and standard deviations for review
print("Overall Means:")
print(overall_means)
print("Overall Standard Deviations:")
print(overall_sd)
```

## Cluster Labeling and Profiles

```{r}
# Add the cluster assignments to the original dataset
lns_subset$cluster <- original_data_with_clusters$cluster

# Label the clusters according to specified orientations
lns_subset$cluster <- factor(lns_subset$cluster,
                             levels = c(1, 2, 3, 4),
                             labels = c("Culture_Affirming", "Assimilationist", "Demicultural", "Bicultural"))

# Decluttering Dataset
lns_subset <- lns_subset %>%
  select(-AMERICAN, -CULTURAL_IDENTITY, -KEEPSPAN, -DISTINCT, -LEARNENG)

str(lns_subset)
```
## RADAR CHART
```{r}
# Load necessary libraries
p_load(mclust, cluster, dplyr, tidyverse, factoextra, flexclust, caret, 
       dunn.test, MASS, Rtsne, NbClust, boot, fpc, ggplot2, gridExtra, grid,
       kableExtra, rstatix, PMCMRplus, FSA, DescTools, stats, purrr, fmsb)

# Compute means for each cluster
cluster_means <- lns_subset %>%
  group_by(cluster) %>%
  summarise(
    American_ID = mean(AMERICAN, na.rm = TRUE),
    Cultural_ID = mean(CULTURAL_IDENTITY, na.rm = TRUE),
    Keep_Spanish = mean(KEEPSPAN, na.rm = TRUE),
    Distinctiveness = mean(DISTINCT, na.rm = TRUE),
    Learn_English = mean(LEARNENG, na.rm = TRUE)
  )

# Set up max/min values for normalization
max_min <- data.frame(
  American_ID = c(5, 1), 
  Cultural_ID = c(5, 1), 
  Keep_Spanish = c(5, 1), 
  Distinctiveness = c(5, 1), 
  Learn_English = c(5, 1)
)

# Bind data together
radar_data <- rbind(max_min, cluster_means)

# Define colors for each cluster
colors <- c("red", "blue", "green", "purple")

# Radar chart visualization
radarchart(radar_data, axistype=1, 
           pcol=colors, plwd=2, plty=1,
           title="Acculturation Orientation Profiles")

legend("topright", legend=unique(lns_subset$cluster), col=colors, lty=1, lwd=2)

```

```{r}

```

```{r}

```

## Save Dataset as clustered_data

```{r}
# Save the updated dataset
save(lns_subset, file = "~/Desktop/Dissertation Project/Data/Processed Data/clustered_data.rda")
```

# End of Script
